{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìà Scraping de Not√≠cias sobre A√ß√µes do site Investing.com com Selenium e Pandas\n",
    "\n",
    "Este script realiza a coleta de not√≠cias sobre a√ß√µes, utilizando **Selenium** para navegar na p√°gina Investing.com e capturar dados din√¢micos. Os dados extra√≠dos s√£o processados com **Pandas** e salvos em um arquivo **CSV** para an√°lise posterior.\n",
    "\n",
    "### Funcionalidade:\n",
    "- **Inicia o WebDriver**: Configura e executa o navegador em modo headless.\n",
    "- **Captura de Not√≠cias**: Coleta not√≠cias da p√°gina de not√≠cias do mercado de a√ß√µes no site \"Investing.com\".\n",
    "- **Extra√ß√£o de Dados**: Extrai informa√ß√µes de cada not√≠cia, como t√≠tulo, descri√ß√£o e data de publica√ß√£o.\n",
    "- **Convers√£o de Tempo**: Converte as informa√ß√µes de tempo relativo (ex: \"5 minutos atr√°s\") para um timestamp absoluto.\n",
    "- **Armazenamento**: Organiza as not√≠cias extra√≠das em um DataFrame do Pandas e as salva em um arquivo CSV.\n",
    "\n",
    "üöÄ **Objetivo**: Automatizar o processo de extra√ß√£o de not√≠cias financeiras e facilitar a an√°lise dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install selenium pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from datetime import datetime\n",
    "import undetected_chromedriver as uc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time_to_timestamp(time_text):\n",
    "    \"\"\"\n",
    "    Converte uma string de tempo relativo (ex: \"7 minutes ago\", \"2 hours ago\") para um timestamp absoluto.\n",
    "    \"\"\"\n",
    "    time_text = time_text.lower()\n",
    "    try:\n",
    "        if 'minute' in time_text:\n",
    "            minutes_ago = int(time_text.split()[0])\n",
    "            return datetime.now() - pd.to_timedelta(minutes_ago, unit='m')\n",
    "        elif 'hour' in time_text:\n",
    "            hours_ago = int(time_text.split()[0])\n",
    "            return datetime.now() - pd.to_timedelta(hours_ago, unit='h')\n",
    "        else:\n",
    "            return pd.to_datetime(time_text)\n",
    "    except:\n",
    "        return None  # Em caso de erro, retorna None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Iniciar ferramenta de web scraping (Selenium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_driver(headless=True, visible=False):\n",
    "    \"\"\"\n",
    "    Inicia o WebDriver do Chrome com prote√ß√£o contra detec√ß√£o de bots (Cloudflare).\n",
    "\n",
    "    Args:\n",
    "        headless (bool): Se True, roda em modo headless (sem interface gr√°fica).\n",
    "        visible (bool): Se True, abre janela gr√°fica para inspe√ß√£o.\n",
    "\n",
    "    Retorna:\n",
    "        uc.Chrome: Inst√¢ncia do driver Chrome com camuflagem.\n",
    "    \"\"\"\n",
    "    options = uc.ChromeOptions()\n",
    "\n",
    "    # Define o modo de exibi√ß√£o\n",
    "    if headless and not visible:\n",
    "        options.add_argument(\"--headless=new\")  # 'new' para melhor compatibilidade\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "    # Define o user-agent (ajuda a parecer um navegador real)\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0 Safari/537.36\")\n",
    "\n",
    "    # Tamanho da janela\n",
    "    if visible:\n",
    "        options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "    # Cria o driver com detec√ß√£o ofuscada\n",
    "    driver = uc.Chrome(options=options)\n",
    "\n",
    "    return driver\n",
    "\n",
    "driver = start_driver(headless=False, visible=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Carregar a p√°gina de not√≠cias do Investing.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_page(driver, url: str, delay: float = 5.0):\n",
    "    \"\"\"\n",
    "    Carrega uma p√°gina e aguarda o carregamento completo.\n",
    "\n",
    "    Args:\n",
    "        driver (webdriver.Chrome): Inst√¢ncia do WebDriver.\n",
    "        url (str): URL a ser carregada.\n",
    "        delay (float): Segundos para aguardar ap√≥s o carregamento.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    driver.get(url)\n",
    "    time.sleep(delay)\n",
    "\n",
    "load_page(driver, \"https://www.investing.com/news/stock-market-news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Obter o nome das classes dos elementos html do site que cont√©m as not√≠cias\n",
    "\n",
    "Obs: Para essa etapa √© necess√°rio inspecionar o elemento html do site, e extrair um trexo no nome da classe que contem a not√≠cia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Classe: news-analysis-v2_content__z0iLP\n",
      "1 - Classe: news-analysis-v2_articles-container__3fFL8\n",
      "2 - Classe: news-analysis-v2_info-item__dOLsl\n",
      "3 - Classe: news-analysis-v2_article__wW0pT\n"
     ]
    }
   ],
   "source": [
    "def get_class_counts(driver, substring: str):\n",
    "    \"\"\"\n",
    "    Filtra classes de elementos cujo nome contenha a substring, imprime √≠ndice e retorna a lista.\n",
    "\n",
    "    Args:\n",
    "        driver (webdriver.Chrome): Inst√¢ncia do WebDriver com p√°gina carregada.\n",
    "        substring (str): Substring a buscar nos nomes de classe.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Lista de nomes de classes que cont√™m a substring.\n",
    "    \"\"\"\n",
    "    # Encontra elementos que tenham a substring em seu atributo class\n",
    "    elements = driver.find_elements(By.CSS_SELECTOR, f\"[class*='{substring}']\")\n",
    "    class_set = set()\n",
    "    for el in elements:\n",
    "        class_attr = el.get_attribute('class') or ''\n",
    "        for cls in class_attr.split():\n",
    "            if substring in cls:\n",
    "                class_set.add(cls)\n",
    "\n",
    "    filtered = list(class_set)\n",
    "    for i, cls in enumerate(filtered):\n",
    "        print(f\"{i} - Classe: {cls}\")\n",
    "    return filtered\n",
    "\n",
    "class_list = get_class_counts(driver, 'news-analysis-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Obter os textos dos elementos html referenciado pelo nome da classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Capturado: 'Republicans split on US credit downgrade as party‚Äôs tax bill lingers\n",
      "By Bo Erickson WASHINGTON (Reuters) -Moody‚Äôs downgrade of the U.S. sovereign credit rating has elicited mixed responses among...\n",
      "By\n",
      "Reuters\n",
      "‚Ä¢\n",
      "35 minutes ago'\n",
      "[DEBUG] Capturado: 'Trump tells Walmart to ‚Äôeat the tariffs‚Äô instead of raising prices\n",
      "By Jasper Ward WASHINGTON (Reuters) -U.S. President Donald Trump said on Saturday that Walmart (N:WMT) should \"eat the tariffs\"...\n",
      "By\n",
      "Reuters\n",
      "‚Ä¢\n",
      "1 hour ago\n",
      "‚Ä¢\n",
      "8'\n",
      "[DEBUG] Capturado: 'Moody‚Äôs cuts America‚Äôs pristine credit rating, citing rising debt\n",
      "By Davide Barbuscia and Pushkala Aripaka (Reuters) -Moody‚Äôs downgraded the U.S. sovereign credit rating on Friday due to concerns...\n",
      "By\n",
      "Reuters\n",
      "‚Ä¢\n",
      "3 hours ago\n",
      "‚Ä¢\n",
      "19'\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "[DEBUG] Capturado: ''\n",
      "Republicans split on US credit downgrade as party‚Äôs tax bill lingers\n",
      "By Bo Erickson WASHINGTON (Reuters) -Moody‚Äôs downgrade of the U.S. sovereign credit rating has elicited mixed responses among...\n",
      "By\n",
      "Reuters\n",
      "‚Ä¢\n",
      "35 minutes ago\n",
      "Trump tells Walmart to ‚Äôeat the tariffs‚Äô instead of raising prices\n",
      "By Jasper Ward WASHINGTON (Reuters) -U.S. President Donald Trump said on Saturday that Walmart (N:WMT) should \"eat the tariffs\"...\n",
      "By\n",
      "Reuters\n",
      "‚Ä¢\n",
      "1 hour ago\n",
      "‚Ä¢\n",
      "8\n",
      "Moody‚Äôs cuts America‚Äôs pristine credit rating, citing rising debt\n",
      "By Davide Barbuscia and Pushkala Aripaka (Reuters) -Moody‚Äôs downgraded the U.S. sovereign credit rating on Friday due to concerns...\n",
      "By\n",
      "Reuters\n",
      "‚Ä¢\n",
      "3 hours ago\n",
      "‚Ä¢\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "def get_texts_by_class(driver, class_name: str):\n",
    "    \"\"\"\n",
    "    Retorna textos de todos os elementos que contenham a classe especificada,\n",
    "    ignorando linhas que sejam apenas n√∫meros, bullet points ou vazias.\n",
    "\n",
    "    Args:\n",
    "        driver (webdriver.Chrome): Inst√¢ncia do WebDriver com p√°gina carregada.\n",
    "        class_name (str): Nome da classe CSS a ser buscada.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Lista de textos v√°lidos dos elementos encontrados.\n",
    "    \"\"\"\n",
    "    selector = f\"[class~='{class_name}']\"\n",
    "    elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "    texts = []\n",
    "\n",
    "    for el in elements:\n",
    "        text = el.text.strip()\n",
    "        print(f\"[DEBUG] Capturado: '{text}'\") \n",
    "        # Ignora se for vazio\n",
    "        if not text:\n",
    "            continue\n",
    "        # Ignora se for apenas n√∫mero\n",
    "        if re.fullmatch(r'\\d+', text):\n",
    "            continue\n",
    "        # Ignora se for bullet point\n",
    "        if text in ['‚Ä¢', '¬∑', '‚óè', '‚ó¶', '-', '‚Äì']:\n",
    "            continue\n",
    "        texts.append(text)\n",
    "\n",
    "    return texts\n",
    "\n",
    "headlines = get_texts_by_class(driver, class_list[3])\n",
    "\n",
    "for line in headlines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Obter os textos relevantes para a classifica√ß√£o do sentimento, como o t√≠tulo, a descri√ß√£o da not√≠cia e a data que ela foi postada\n",
    "\n",
    "Obs: **An√°lise** a linhas da lista obtida anteriormente e **ajuste** os parametros de acordo para a leitura correta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'titulo': 'Republicans split on US credit downgrade as party‚Äôs tax bill lingers\\nBy Bo Erickson WASHINGTON (Reuters) -Moody‚Äôs downgrade of the U.S. sovereign credit rating has elicited mixed responses among...\\nBy\\nReuters\\n‚Ä¢\\n13 minutes ago',\n",
       "  'descricao': 'Trump tells Walmart to ‚Äôeat the tariffs‚Äô instead of raising prices\\nBy Jasper Ward WASHINGTON (Reuters) -U.S. President Donald Trump said on Saturday that Walmart (N:WMT) should \"eat the tariffs\"...\\nBy\\nReuters\\n‚Ä¢\\n1 hour ago\\n‚Ä¢\\n8',\n",
       "  'data': None}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_time_to_timestamp(time_text):\n",
    "    \"\"\"\n",
    "    Converte uma string de tempo relativo (ex: \"7 minutes ago\", \"2 hours ago\") para um timestamp absoluto.\n",
    "    \"\"\"\n",
    "    time_text = time_text.lower()\n",
    "    try:\n",
    "        if 'minute' in time_text:\n",
    "            minutes_ago = int(time_text.split()[0])\n",
    "            return datetime.now() - pd.to_timedelta(minutes_ago, unit='m')\n",
    "        elif 'hour' in time_text:\n",
    "            hours_ago = int(time_text.split()[0])\n",
    "            return datetime.now() - pd.to_timedelta(hours_ago, unit='h')\n",
    "        else:\n",
    "            return pd.to_datetime(time_text)\n",
    "    except:\n",
    "        return None  # Em caso de erro, retorna None\n",
    "\n",
    "def parse_news_items(headlines: list[str], start_line: int, end_line: int, lines_per_news: int = 2,\n",
    "                     titulo_offset: int = 0, descricao_offset: int = 1, data_offset: int | None = None) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Remove os 'start_line' primeiros e 'end_line' √∫ltimos elementos da lista e agrupa de n em n, extraindo t√≠tulo, descri√ß√£o e data.\n",
    "\n",
    "    Args:\n",
    "        headlines (list[str]): Lista de textos capturados pelo get_texts_by_class.\n",
    "        start_line (int): Quantos itens remover do in√≠cio.\n",
    "        end_line (int): Quantos itens remover do final.\n",
    "        lines_per_news (int): Tamanho do agrupamento (padr√£o 2 = t√≠tulo + descri√ß√£o).\n",
    "        titulo_offset (int): √çndice relativo dentro do grupo para o t√≠tulo.\n",
    "        descricao_offset (int): √çndice relativo dentro do grupo para a descri√ß√£o.\n",
    "        data_offset (int | None): √çndice relativo dentro do grupo para a data. Se None, a data n√£o ser√° extra√≠da.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: Lista de dicion√°rios com 'titulo', 'descricao' e 'data' (se dispon√≠vel).\n",
    "    \"\"\"\n",
    "    trimmed = headlines[start_line:len(headlines) - end_line]\n",
    "    noticias = []\n",
    "\n",
    "    for i in range(0, len(trimmed) - (lines_per_news - 1), lines_per_news):\n",
    "        grupo = trimmed[i:i + lines_per_news]\n",
    "        if len(grupo) == lines_per_news:\n",
    "            try:\n",
    "                titulo = grupo[titulo_offset].strip()\n",
    "                descricao = grupo[descricao_offset].strip()\n",
    "                data = None\n",
    "\n",
    "                if data_offset is not None and 0 <= data_offset < len(grupo):\n",
    "                    raw_data = grupo[data_offset].strip()\n",
    "                    data = convert_time_to_timestamp(raw_data)\n",
    "\n",
    "                noticias.append({\n",
    "                    \"titulo\": titulo,\n",
    "                    \"descricao\": descricao,\n",
    "                    \"data\": data\n",
    "                })\n",
    "            except IndexError:\n",
    "                continue  # pula grupos mal formados\n",
    "\n",
    "    return noticias\n",
    "\n",
    "news = parse_news_items(headlines, start_line=0, end_line=0, lines_per_news=3, title_offset=0, description_offset=1)\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_string(html_string, keyword, start_char, end_char, division_index: int=1):\n",
    "    \"\"\"\n",
    "    Extrai um trecho de texto de uma string HTML com base em uma palavra-chave e os caracteres delimitadores.\n",
    "\n",
    "    Args:\n",
    "        html_string (str): A string HTML de onde extrair o texto.\n",
    "        keyword (str): Palavra-chave para localizar o ponto de extra√ß√£o.\n",
    "        start_char (str): Caractere de in√≠cio do trecho a ser extra√≠do.\n",
    "        end_char (str): Caractere de fim do trecho a ser extra√≠do.\n",
    "        division_index (int, opcional): √çndice para dividir a string HTML (padr√£o √© 1).\n",
    "\n",
    "    Retorna:\n",
    "        str: O texto extra√≠do, ou None se a extra√ß√£o falhar.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Divide o HTML pela palavra-chave\n",
    "        parts = html_string.split(keyword)\n",
    "        # print(f\"Parts[1] ({len(parts)}) :\", parts[1])\n",
    "        \n",
    "        # Verifica se a palavra-chave est√° no HTML\n",
    "        if len(parts) < 2:\n",
    "            print(f'({len(parts)})\\n\\n({keyword})\\n{parts}\\n\\n')\n",
    "            return None\n",
    "        \n",
    "        # Pega a segunda parte da divis√£o\n",
    "        text_part = parts[division_index]\n",
    "        \n",
    "        # Encontra o √≠ndice do caractere inicial\n",
    "        start_index = text_part.find(start_char)\n",
    "        \n",
    "        # Verifica se o caractere inicial foi encontrado\n",
    "        if start_index == -1:\n",
    "            return None\n",
    "        \n",
    "        # Ajusta o √≠ndice inicial para come√ßar ap√≥s o caractere inicial\n",
    "        start_index += len(start_char)\n",
    "        \n",
    "        # Encontra o √≠ndice do caractere final a partir do √≠ndice inicial\n",
    "        end_index = text_part.find(end_char, start_index)\n",
    "        \n",
    "        # Verifica se o caractere final foi encontrado\n",
    "        if end_index == -1:\n",
    "            return None\n",
    "        \n",
    "        # Extrai o texto entre o caractere inicial e o caractere final\n",
    "        extracted_text = text_part[start_index:end_index]\n",
    "        \n",
    "        return extracted_text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao extrair texto: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para extrair dados de um item de not√≠cia\n",
    "def extract_news_item_data(item):\n",
    "    \"\"\"\n",
    "    Extrai dados (link, t√≠tulo, descri√ß√£o, data de publica√ß√£o) de um item de not√≠cia.\n",
    "\n",
    "    Args:\n",
    "        item (str): String HTML representando um item de not√≠cia.\n",
    "\n",
    "    Retorna:\n",
    "        dict: Dicion√°rio contendo 'Link', 'Title', 'Description' e 'PublishDate'.\n",
    "    \"\"\"\n",
    "    # print(f'Item:\\n\\n{textwrap.indent(item.get_attribute(\"innerHTML\"), prefix='    ')}\\n\\n')\n",
    "    try:        \n",
    "        # Extraindo o link do artigo\n",
    "        link = extract_text_string(item, \"article-title-link\", 'href=\"', '\"', division_index=0)\n",
    "        print(f'Article Link: {link}')\n",
    "\n",
    "        # Extraindo o t√≠tulo do artigo\n",
    "        title = extract_text_string(item, \"article-title-link\", \">\", \"<\")\n",
    "        print(f'Article Title: {title}')\n",
    "\n",
    "        # Extraindo a descri√ß√£o do artigo\n",
    "        description = extract_text_string(item, \"article-description\", \">\", \"<\")\n",
    "        print(f'Article Description: {description}')\n",
    "\n",
    "        # Extraindo a data de publica√ß√£o\n",
    "        publish_date = extract_text_string(item, \"article-publish-date\", 'datetime=\"', '\"')\n",
    "        print(f'Publish Date: {publish_date}')\n",
    "\n",
    "        return {\n",
    "            'Link': link,\n",
    "            'Title': title,\n",
    "            'Description': description,\n",
    "            'PublishDate': publish_date\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar item: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados salvos em 'news_data.csv'\n"
     ]
    }
   ],
   "source": [
    "# Fun√ß√£o para extrair os dados das not√≠cias\n",
    "def extract_news_data(news_items):\n",
    "    \"\"\"\n",
    "    Extrai dados de uma lista de itens de not√≠cias e os organiza em um DataFrame.\n",
    "\n",
    "    Args:\n",
    "        news_items (list): Lista de strings HTML contendo os itens de not√≠cias.\n",
    "\n",
    "    Retorna:\n",
    "        pd.DataFrame: DataFrame contendo os dados extra√≠dos (link, t√≠tulo, descri√ß√£o, data de publica√ß√£o).\n",
    "    \"\"\"\n",
    "    # Lista para armazenar os dados\n",
    "    news_data = []\n",
    "\n",
    "    # Itera sobre os itens de not√≠cias\n",
    "    for item in news_items:\n",
    "        if item.startswith('<li class=\"list_list__item__dwS6E !mt-0 border-t'):\n",
    "            news_item_data = extract_news_item_data(item)\n",
    "            if news_item_data:\n",
    "                news_data.append(news_item_data)\n",
    "\n",
    "    # Cria um DataFrame com os dados extra√≠dos\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "\n",
    "    return news_df\n",
    "\n",
    "news_df = extract_news_data(news_items)\n",
    "news_df.to_csv('news_data.csv', index=False)\n",
    "print(\"Dados salvos em 'news_data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_save_news():\n",
    "    \"\"\"\n",
    "    Fun√ß√£o principal que executa o scraping das not√≠cias, processa os dados e os salva em um arquivo CSV.\n",
    "\n",
    "    Salva os dados em 'news_data.csv', adicionando dados se o arquivo j√° existir.\n",
    "    \"\"\"\n",
    "    driver = start_driver()\n",
    "    news_df = extract_news_data(driver)\n",
    "    driver.quit()  # Fecha o navegador\n",
    "    \n",
    "    # Verifica se o arquivo j√° existe\n",
    "    file_exists = os.path.isfile('news_data.csv')\n",
    "    \n",
    "    # Salva o DataFrame em um arquivo CSV, adicionando dados se o arquivo j√° existir\n",
    "    news_df.to_csv('news_data.csv', mode='a', header=not file_exists, index=False)\n",
    "    print(\"Dados adicionados em 'news_data.csv'\")\n",
    "    \n",
    "# Chama a fun√ß√£o principal\n",
    "# scrape_and_save_news()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
